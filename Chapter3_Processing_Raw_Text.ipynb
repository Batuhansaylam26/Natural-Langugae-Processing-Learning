{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Hugging Face API Token ile giriş\n",
    "login(token=\"***\")\n",
    "\n",
    "# Veri setini yükleme\n",
    "ds = load_dataset(\"bananabot/TrumpSpeeches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=ds[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My fellow Americans, I want to speak to you to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Making America Great Again has always been abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tragically, over the course of the past year m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Now I am asking everyone who has ever believed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                                   \n",
       "1  My fellow Americans, I want to speak to you to...\n",
       "2  Making America Great Again has always been abo...\n",
       "3  Tragically, over the course of the past year m...\n",
       "4  Now I am asking everyone who has ever believed..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " My fellow Americans, I want to speak to you tonight about the troubling events of the past week. As I have said, the incursion of the US Capitol struck at the very heart of our Republic. It angered and appalled millions of Americans across the political spectrum. I want to be very clear, I unequivocally condemn the violence that we saw last week. Violence and vandalism have absolutely no place in our country and no place in our movement. Making America Great Again has always been about defendin\n"
     ]
    }
   ],
   "source": [
    "txt=\" \".join(ds[\"text\"].astype(str))\n",
    "print(txt[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokens = nltk.word_tokenize(txt)\n",
    "corpus = nltk.Text(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: My fellow Americans , I want to speak...>\n"
     ]
    }
   ],
   "source": [
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Common structures for text corpora: The simplest kind of corpus is a collection of isolated\n",
    " texts with no particular organization; some corpora are structured into categories, such as genre\n",
    " (Brown Corpus); some categorizations overlap, such as topic categories (Reuters Corpus); other\n",
    " corpora represent language use over time (Inaugural Address Corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **THE NLP PIPELINE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) HTML (Download web page, strip HTML if necessary, trim to desired content)\n",
    "2) ASCII (Tokenize the text, select tokens of interest,create an NLTK text)\n",
    "3) Text \n",
    "4) Vocab (Normalize the words, build the vocabulary)\n",
    "\n",
    " The processing pipeline: We open a URL and read its HTML content, remove the markup\n",
    " and select a slice of characters; this is then tokenized and optionally converted into an nltk.Text\n",
    " object; we can also lowercase all the words and extract the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **Text Processing with Unicode**\n",
    " Our programs will often need to deal with different languages, and different character\n",
    " sets.  \n",
    " If you live in the English-speaking world\n",
    " you probably use ASCII, possibly without realizing it.  \n",
    "\n",
    " If you live in Europe you might\n",
    " use one of the extended Latin character sets.\n",
    "  In this section, we will give an overview of how to use Unicode for\n",
    " processing texts that use non-ASCII character sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Unicode?\n",
    "\n",
    " Unicode supports over a million characters. Each character is assigned a number, called\n",
    " a **code point**. In Python, code points are written in the form \\uXXXX, where XXXX\n",
    " is the number in ***four-digit hexadecimal form***.\n",
    "\n",
    " Text in files will be in a particular encoding, so we need some mechanism for translating\n",
    " it into Unicode—translation into Unicode is called ***decoding***. Conversely, to write out\n",
    " Unicode to a file or a terminal, we first need to translate it into a suitable ***encoding***—\n",
    " this translation out of Unicode is called encoding.\n",
    "\n",
    " From a Unicode perspective, characters are abstract entities that can be realized as one\n",
    " or more **glyphs**. Only glyphs can appear on a screen or be printed on paper. A font is\n",
    " a mapping from characters to glyphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regular Expressions for Detecting Word Patterns**\n",
    "\n",
    "Many linguistic processing tasks involve pattern matching.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
